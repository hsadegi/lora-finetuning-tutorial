# Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ LoRA Fine-tuning ðŸš€

**Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ø§ ØªÚ©Ù†ÛŒÚ© LoRA**

## ðŸ“‹ ÙÙ‡Ø±Ø³Øª Ù…Ø·Ø§Ù„Ø¨

- [Ù…Ù‚Ø¯Ù…Ù‡](#Ù…Ù‚Ø¯Ù…Ù‡)
- [Ù†ØµØ¨ Ùˆ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ](#Ù†ØµØ¨-Ùˆ-Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ)
- [Ø¯Ø±Ú© LoRA](#Ø¯Ø±Ú©-lora)
- [Ø¢Ù…ÙˆØ²Ø´ Ù‚Ø¯Ù… Ø¨Ù‡ Ù‚Ø¯Ù…](#Ø¢Ù…ÙˆØ²Ø´-Ù‚Ø¯Ù…-Ø¨Ù‡-Ù‚Ø¯Ù…)
- [Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ](#Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ-Ø¹Ù…Ù„ÛŒ)
- [Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù†Ú©Ø§Øª](#Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ-Ùˆ-Ù†Ú©Ø§Øª)

## ðŸŽ¯ Ù…Ù‚Ø¯Ù…Ù‡

**LoRA (Low-Rank Adaptation)** ÛŒÚ©ÛŒ Ø§Ø² Ù…ÙˆØ«Ø±ØªØ±ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ Ø¨Ø²Ø±Ú¯ Ø§Ø³Øª Ú©Ù‡:

- âœ… **99% Ú©Ø§Ù‡Ø´** Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´
- âœ… **Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§** Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø³ØªÙ†ØªØ§Ø¬
- âœ… **Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡** Ø§Ø² Ø­Ø§ÙØ¸Ù‡ GPU
- âœ… **Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø´Ø§Ø¨Ù‡** Fine-tuning Ú©Ø§Ù…Ù„

## ðŸ”§ Ù†ØµØ¨ Ùˆ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ

### Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§
```bash
Python >= 3.8
CUDA >= 11.0 (Ø¨Ø±Ø§ÛŒ GPU)
RAM >= 8GB
GPU Memory >= 4GB (ØªÙˆØµÛŒÙ‡ Ø´Ø¯Ù‡)
```

### Ù†ØµØ¨ Ù¾Ú©ÛŒØ¬â€ŒÙ‡Ø§
```bash
pip install -r requirements.txt
```

## ðŸ§  Ø¯Ø±Ú© LoRA

### Ú†Ú¯ÙˆÙ†Ù‡ LoRA Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ

```python
# Ø¨Ø¬Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ú©Ù„ Ù…Ø§ØªØ±ÛŒØ³ W (Ø¨Ø²Ø±Ú¯ Ùˆ Ù¾Ø±Ù‡Ø²ÛŒÙ†Ù‡)
W_updated = W_original + Î”W

# LoRA Î”W Ø±Ø§ Ø¨Ù‡ Ø¯Ùˆ Ù…Ø§ØªØ±ÛŒØ³ Ú©ÙˆÚ†Ú© ØªØ¬Ø²ÛŒÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
Î”W = A Ã— B
# Ø¬Ø§ÛŒÛŒ Ú©Ù‡ A: [d, r] Ùˆ B: [r, d] Ùˆ r << d
```

### Ù…Ø²Ø§ÛŒØ§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:
- ðŸ”¹ **Ú©Ø§Ù‡Ø´ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§**: Ø§Ø² Ù…ÛŒÙ„ÛŒÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ Ù‡Ø²Ø§Ø±Ù‡Ø§
- ðŸ”¹ **Ø³Ø±Ø¹Øª Ø¢Ù…ÙˆØ²Ø´**: 3-5 Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ±
- ðŸ”¹ **Ø­Ø§ÙØ¸Ù‡ Ú©Ù…ØªØ±**: Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø±ÙˆÛŒ GPUâ€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©
- ðŸ”¹ **Ù…Ø¯ÙˆÙ„Ø§Ø±**: Ù‚Ø§Ø¨Ù„ ØªØ±Ú©ÛŒØ¨ Ùˆ ØªØ¹ÙˆÛŒØ¶

## ðŸš€ Ø¢Ù…ÙˆØ²Ø´ Ù‚Ø¯Ù… Ø¨Ù‡ Ù‚Ø¯Ù…

### Ú¯Ø§Ù… Û±: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡
model_name = "microsoft/DialoGPT-medium"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

### Ú¯Ø§Ù… Û²: ØªÙ†Ø¸ÛŒÙ… LoRA
```python
# Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ LoRA
lora_config = LoraConfig(
    r=16,                    # Ø±ØªØ¨Ù‡ ØªØ¬Ø²ÛŒÙ‡ (Ú©Ù„ÛŒØ¯ Ø§ØµÙ„ÛŒ)
    lora_alpha=32,          # Ø¶Ø±ÛŒØ¨ Ù…Ù‚ÛŒØ§Ø³â€ŒØ¨Ù†Ø¯ÛŒ
    target_modules=["c_attn", "c_proj"],  # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù‡Ø¯Ù
    lora_dropout=0.1,       # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting
    bias="none",            # Ù†ÙˆØ¹ bias
    task_type="CAUSAL_LM"   # Ù†ÙˆØ¹ Ú©Ø§Ø±
)

# Ø§Ø¹Ù…Ø§Ù„ LoRA Ø¨Ù‡ Ù…Ø¯Ù„
model = get_peft_model(model, lora_config)
```

### Ú¯Ø§Ù… Û³: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
```python
# Ù…Ø«Ø§Ù„ Ø³Ø§Ø¯Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯ÙØªÚ¯Ùˆ
conversations = [
    {"input": "Ø³Ù„Ø§Ù…ØŒ Ø­Ø§Ù„ Ø´Ù…Ø§ Ú†Ø·ÙˆØ± Ø§Ø³ØªØŸ", "output": "Ø³Ù„Ø§Ù…! Ù…Ù† Ø®ÙˆØ¨Ù…ØŒ Ù…ØªØ´Ú©Ø±Ù…. Ø´Ù…Ø§ Ú†Ø·ÙˆØ±ÛŒØ¯ØŸ"},
    {"input": "Ø§Ù…Ø±ÙˆØ² Ù‡ÙˆØ§ Ú†Ø·ÙˆØ± Ø§Ø³ØªØŸ", "output": "Ù‡ÙˆØ§ Ø¢ÙØªØ§Ø¨ÛŒ Ùˆ Ø¯Ù„Ù¾Ø°ÛŒØ± Ø§Ø³Øª. Ø±ÙˆØ² Ø®ÙˆØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ±ÙˆÛŒ!"}
]

# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ø¢Ù…ÙˆØ²Ø´
def prepare_dataset(conversations):
    inputs, outputs = [], []
    for conv in conversations:
        inputs.append(conv["input"])
        outputs.append(conv["output"])
    return inputs, outputs
```

### Ú¯Ø§Ù… Û´: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
```python
from transformers import Trainer, TrainingArguments

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´
training_args = TrainingArguments(
    output_dir="./lora_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    warmup_steps=100,
    learning_rate=2e-4,
    fp16=True,                    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² precision Ú©Ù…ØªØ±
    logging_steps=10,
    save_strategy="epoch"
)

# Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer
)

trainer.train()
```

## ðŸ“Š Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ

### Û±. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ú¯ÙØªÚ¯Ùˆ ÙØ§Ø±Ø³ÛŒ
```python
# Ø§Ø¬Ø±Ø§ÛŒ notebook Ø¢Ù…ÙˆØ²Ø´ÛŒ
jupyter notebook notebooks/persian_chat_lora.ipynb
```

### Û². Fine-tuning Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
```python
# Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªØ®ØµØµÛŒ
python src/summarization_lora.py
```

### Û³. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø³ÙˆØ§Ù„-Ù¾Ø§Ø³Ø®
```python
# Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Q&A
python src/qa_lora.py --dataset_path data/qa_dataset.json
```

## âš¡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù†Ú©Ø§Øª

### Ù†Ú©Ø§Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ÛŒ:
```python
# Û±. Ø§Ù†ØªØ®Ø§Ø¨ r Ù…Ù†Ø§Ø³Ø¨
r = 8    # Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ù‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡
r = 16   # Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø«Ø± Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ (ØªÙˆØµÛŒÙ‡ Ø´Ø¯Ù‡)
r = 32   # Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡

# Û². Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
torch.backends.cudnn.benchmark = True
model.gradient_checkpointing_enable()

# Û³. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² QLoRA Ø¨Ø±Ø§ÛŒ GPU Ú©ÙˆÚ†Ú©
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)
```

### Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ø±Ø§ÛŒØ¬:
- ðŸ”§ **CUDA out of memory**: Ú©Ø§Ù‡Ø´ batch_size ÛŒØ§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² gradient_checkpointing
- ðŸ”§ **Ø¢Ù…ÙˆØ²Ø´ Ø¢Ù‡Ø³ØªÙ‡**: Ø§ÙØ²Ø§ÛŒØ´ batch_size ÛŒØ§ gradient_accumulation_steps
- ðŸ”§ **Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙ**: Ø§ÙØ²Ø§ÛŒØ´ r ÛŒØ§ ØªØºÛŒÛŒØ± target_modules

## ðŸ“ˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯

```python
# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ù…Ø¯Ù„
from src.evaluation import evaluate_model

results = evaluate_model(
    model=trained_model,
    test_dataset=test_data,
    metrics=["bleu", "rouge", "perplexity"]
)

print(f"BLEU Score: {results['bleu']:.3f}")
print(f"ROUGE Score: {results['rouge']:.3f}")
print(f"Perplexity: {results['perplexity']:.3f}")
```

## ðŸŽ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒÛŒØ§ÙØªÙ‡

```python
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")
model = PeftModel.from_pretrained(base_model, "./lora_model")

# ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
def generate_response(prompt):
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(inputs, max_length=100, do_sample=True)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ØªØ³Øª
response = generate_response("Ø³Ù„Ø§Ù…ØŒ Ø§Ù…Ø±ÙˆØ² Ú†Ù‡ Ø®Ø¨Ø±ØŸ")
print(response)
```

## ðŸ“š Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø¶Ø§ÙÛŒ

- ðŸ“– [Ù…Ù‚Ø§Ù„Ù‡ Ø§ØµÙ„ÛŒ LoRA](https://arxiv.org/abs/2106.09685)
- ðŸ”— [Ù…Ø³ØªÙ†Ø¯Ø§Øª PEFT](https://huggingface.co/docs/peft)
- ðŸ’¡ [Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±](examples/)
- ðŸŽ¥ [ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ](docs/videos.md)

## ðŸ¤ Ù…Ø´Ø§Ø±Ú©Øª

Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ø±Ú©Øª Ø¯Ø± Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡:
1. Fork Ú©Ù†ÛŒØ¯
2. Branch Ø¬Ø¯ÛŒØ¯ Ø¨Ø³Ø§Ø²ÛŒØ¯
3. ØªØºÛŒÛŒØ±Ø§Øª Ø±Ø§ commit Ú©Ù†ÛŒØ¯
4. Pull Request Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯

## ðŸ“ž Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ

Ø³ÙˆØ§Ù„Ø§Øª Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± [Issues](../../issues) Ù…Ø·Ø±Ø­ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø¨Ø§ Ù…Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§Ø´ÛŒØ¯.

---

**Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ â¤ï¸ Ø¨Ø±Ø§ÛŒ Ø¬Ø§Ù…Ø¹Ù‡ AI Ø§ÛŒØ±Ø§Ù†**
